# 1. 流程图

![](/home/colby/myntai_work_note/00.kernel-dri-frame/10.camera/00.Android USB Camera/02.v4l2应用层编程.assets/2019-09-25 18-39-55 的屏幕截图.png)

# 2. hal层分析

## 1. 打开摄像头

```c++
// hardware/libcamera$ vim CameraHardware.cpp
CameraHardware::CameraHardware(const hw_module_t* module, char* devLocation) : mWin(0), ...{
  
 		mVideoDevice = devLocation; // 这个地方传入的是，打开那个摄像头 mVideoDevice =/dev/video0
    
    	PowerOn(); // 这里并没有真正的去打开 
    
    	initDefaultParameters(); // 在这个里面打开的
}

```

```shell
initDefaultParameters()
	-> camera.Open(mVideoDevice) # 这里打开了 涉嫌头  V4L2Camera   camera;
```



```c++
//V4L2Camera.cpp
int V4L2Camera::Open (const char *device)
{
    int ret;

    /* Close the previous instance, if any */
    Close();

    memset(videoIn, 0, sizeof (struct vdIn));

    fd = open(device, O_RDWR);

    ret = ioctl (fd, VIDIOC_QUERYCAP, &videoIn->cap);

    if ((videoIn->cap.capabilities & V4L2_CAP_VIDEO_CAPTURE) == 0) {
        ALOGE("Error opening device: video capture not supported.");
        return -1;
    }

    if (!(videoIn->cap.capabilities & V4L2_CAP_STREAMING)) {
        ALOGE("Capture device does not support streaming i/o");
        return -1;
    }

    /* Enumerate all available frame formats */
    EnumFrameFormats();

    return ret;                                                                                                                                                                                                    
}
```

打开之后，干了两件事 

1. VIDIOC_QUERYCAP 查询驱动功能
   1. 是否支持 V4L2_CAP_VIDEO_CAPTURE 拍照
   2. 是否支持 V4L2_CAP_STREAMING 录像
2. 枚举 所有帧的格式

## 2. VIDIOC_QUERYCAP 

打开摄像头后，紧接着就是 查询驱动的能力，看是否支持 拍照和摄像，之后枚举摄像头所支持的帧的格式

```shell
V4L2Camera::Open
	-> open(device, O_RDWR);
	-> ioctl (fd, VIDIOC_QUERYCAP, &videoIn->cap);
	-> EnumFrameFormats()
		-> ioctl(fd,VIDIOC_ENUM_FMT, &fmt)
		-> EnumFrameSizes() # 例如yuyv 格式所对应的 帧的大小
```

log打印如下：

```shell
V4L2Camera::EnumFrameFormats                                                                                                                                                                                   
{ pixelformat = 'YUYV', description = 'YUYV 4:2:2' }
V4L2Camera::EnumFrameSizes: pixfmt: 0x56595559
{ discrete: width = 640, height = 480 }
V4L2Camera::EnumFrameIntervals: pixfmt: 0x56595559, w:640, h:480
    Time interval between frame: 
1/30
{ discrete: width = 160, height = 120 }
V4L2Camera::EnumFrameIntervals: pixfmt: 0x56595559, w:160, h:120
    Time interval between frame: 
1/30
{ discrete: width = 176, height = 144 }
V4L2Camera::EnumFrameIntervals: pixfmt: 0x56595559, w:176, h:144
    Time interval between frame: 
1/30
{ discrete: width = 320, height = 240 }
V4L2Camera::EnumFrameIntervals: pixfmt: 0x56595559, w:320, h:240
    Time interval between frame: 
1/30
{ discrete: width = 352, height = 288 }
V4L2Camera::EnumFrameIntervals: pixfmt: 0x56595559, w:352, h:288
    Time interval between frame: 
1/30
V4L2Camera::getAvailableSizes
V4L2Camera::getAvailableFps
```



### 1. VIDIOC_ENUM_FMT

这儿的格式有 yuyv   mjepg等格式的

```cpp
bool V4L2Camera::EnumFrameFormats()
{
    ALOGD("V4L2Camera::EnumFrameFormats");
    struct v4l2_fmtdesc fmt;
                                                                                                                                                                                                                   
    m_AllFmts.clear();

    memset(&fmt, 0, sizeof(fmt));
    fmt.index = 0;
    fmt.type = V4L2_BUF_TYPE_VIDEO_CAPTURE;

    while (ioctl(fd,VIDIOC_ENUM_FMT, &fmt) >= 0) {
        fmt.index++;
        ALOGD("{ pixelformat = '%c%c%c%c', description = '%s' }",
                fmt.pixelformat & 0xFF, (fmt.pixelformat >> 8) & 0xFF,
                (fmt.pixelformat >> 16) & 0xFF, (fmt.pixelformat >> 24) & 0xFF,
                fmt.description);  /* { pixelformat = 'YUYV', description = 'YUYV 4:2:2' }*/

        //enumerate frame sizes for this pixel format
        if (!EnumFrameSizes(fmt.pixelformat)) { //在 m_AllFmts 中添加  SurfaceDesc
            ALOGE("  Unable to enumerate frame sizes.");
        }
    };

    // Now, select the best preview format and the best PictureFormat
    // 现在，选择最佳预览格式和最佳图片格式
    m_BestPreviewFmt = SurfaceDesc(); // new 了一个新的 用于 预览格式
    m_BestPictureFmt = SurfaceDesc(); // new 了一个新的用于 拍照

    unsigned int i;
    for (i=0; i<m_AllFmts.size(); i++) {
        SurfaceDesc s = m_AllFmts[i];

        // Prioritize size over everything else when taking pictures. use the least fps possible, as that usually means better quality 
        // 拍照时把大小放在首位。使用尽可能少的fps，因为这通常意味着更好的质量
        if ((s.getSize()  > m_BestPictureFmt.getSize()) ||
            (s.getSize() == m_BestPictureFmt.getSize() && s.getFps() < m_BestPictureFmt.getFps() )
            ) {
            // 选择尺寸最大的， 在相投大小下，选择fps最小的，这样拍出的照片，质量是最好的
            m_BestPictureFmt = s;
        }

        // Prioritize fps, then size when doing preview
        // 按优先级排列fps，然后在预览时调整大小
        if ((s.getFps()  > m_BestPreviewFmt.getFps()) ||
            (s.getFps() == m_BestPreviewFmt.getFps() && s.getSize() > m_BestPreviewFmt.getSize() )
            ) {
            // 选择 fps最大的， 在相同fps下，选择尺寸最大的
            m_BestPreviewFmt = s;
        }

    }

    return true;
}
```

#### 1.  VIDIOC_ENUM_FRAMESIZES

```cpp
bool V4L2Camera::EnumFrameSizes(int pixfmt)
{
    ALOGD("V4L2Camera::EnumFrameSizes: pixfmt: 0x%08x",pixfmt);
    memset(&fsize, 0, sizeof(fsize));
    fsize.index = 0;
    fsize.pixel_format = pixfmt;
    while (ioctl(fd, VIDIOC_ENUM_FRAMESIZES, &fsize) >= 0) {
        fsize.index++;
        if (fsize.type == V4L2_FRMSIZE_TYPE_DISCRETE) {
            ALOGD("{ discrete: width = %u, height = %u }",
                fsize.discrete.width, fsize.discrete.height);// { discrete: width = 640, height = 480 }

            fsizeind++;

            if (!EnumFrameIntervals(pixfmt,fsize.discrete.width, fsize.discrete.height))
                ALOGD("  Unable to enumerate frame intervals");
        	}else if(){
            
       		 }....
    }
}
```



##### 1. VIDIOC_ENUM_FRAMEINTERVALS

枚举每个帧对应的 fps

```cpp
/*
	V4L2Camera::EnumFrameIntervals: pixfmt: 0x56595559, w:640, h:480
    Time interval between frame: 
	1/30
*/

bool V4L2Camera::EnumFrameIntervals(int pixfmt, int width, int height)
{
    ALOGD("V4L2Camera::EnumFrameIntervals: pixfmt: 0x%08x, w:%d, h:%d",pixfmt,width,height);
    struct v4l2_frmivalenum fival;                                                                                                                                                                                 
    int list_fps=0;
    memset(&fival, 0, sizeof(fival));
    fival.index = 0;
    fival.pixel_format = pixfmt;
    fival.width = width;
    fival.height = height;

    ALOGD("\tTime interval between frame: ");
    while (ioctl(fd,VIDIOC_ENUM_FRAMEINTERVALS, &fival) >= 0)
    {
        fival.index++;
        if (fival.type == V4L2_FRMIVAL_TYPE_DISCRETE) {                                                                                                                                                            
            ALOGD("%u/%u", fival.discrete.numerator, fival.discrete.denominator); // 1 / 30

           /////////////////////////////////////////////////////////////////////////////////
            /* 这个时对特定的 设备进行 fps的过滤 */
            if (strstr((const char *)videoIn->cap.card, "C170")) {
                ALOGD("Special mode %s", videoIn->cap.card);
                if (width != 640 || height != 360 || fival.discrete.denominator != 30) {
                    ALOGD("Skip %dx%d@%d", width, height, fival.discrete.denominator);
                    continue;
                }
                ALOGD("Special mode found %dx%d@%d", width, height, fival.discrete.denominator);
            }
            /////////////////////////////////////////////////////////////////////////////////

            m_AllFmts.add( SurfaceDesc( width, height, fival.discrete.denominator ) ); // 这里会 new SurfaceDesc
            list_fps++;
        }
        
        return true;
    }
```

这里会将 fps 放在 SurfaceDesc 中 ，并添加到 m_AllFmts 中

## 3. 开始预览

设置相机的格式：

```cpp
 CameraHardware::setPreviewWindow
 CameraHardware::startPreview
  CameraHardware::startPreviewLocked
  CameraHardware::startPreviewLocked: Open, 640x480
		-> V4L2Camera::EnumFrameFormats  
      
 CameraHardware::startPreviewLocked: Init
		-> V4L2Camera::Init()
    			-> ioctl(fd, VIDIOC_S_FMT, &videoIn->format)
     			-> ioctl (fd, VIDIOC_QUERYBUF, &videoIn->buf); // 申请缓存
				-> mmap() // 映射到用户空间
                ->  ioctl(fd, VIDIOC_QBUF, &videoIn->buf) // 加入kenrl 中的　缓存队列中，准备采集
     	-> initHeapLocked
     	-> camera.StartStreaming();
```

```cpp
// CameraHardware.cpp
status_t CameraHardware::startPreviewLocked()
{
    ALOGD("CameraHardware::startPreviewLocked");
    
    // If we are recording, use the recording video size instead of the preview size
    // 如果我们 有记录，那么使用 记录的video size 替代 preview size
    if (mRecordingEnabled && mMsgEnabled & CAMERA_MSG_VIDEO_FRAME) {
        mParameters.getVideoSize(&width, &height);
    } else {
        mParameters.getPreviewSize(&width, &height);
    }
    
    
    int fps = mParameters.getPreviewFrameRate();
    
    ALOGD("CameraHardware::startPreviewLocked: Open, %dx%d", width, height);
    status_t ret = camera.Open(mVideoDevice);
    
    ALOGD("CameraHardware::startPreviewLocked: Init");
    ret = camera.Init(width, height, fps); // 这里会 去设置 VIDIOC_S_FMT
    
    
    /* Retrieve the real size being used */
    camera.getSize(width, height);
    ALOGD("CameraHardware::startPreviewLocked: effective size: %dx%d",width, height);  // 这里将打印最终设置的 大小                                                                                                                        

    // If we are recording, use the recording video size instead of the preview size
    // 这里是对 大小做 保存
    if (mRecordingEnabled && mMsgEnabled & CAMERA_MSG_VIDEO_FRAME) {
        /* Store it as the video size to use */
        mParameters.setVideoSize(width, height);
    } else {
        /* Store it as the preview size to use */
        mParameters.setPreviewSize(width, height);
    }
    
    
    /* And reinit the memory heaps to reflect the real used size if needed */
   	// 如果需要的话，重新初始化内存堆以反映实际使用的大小
    initHeapLocked();
    
    
    ALOGD("CameraHardware::startPreviewLocked: StartStreaming");
    ret = camera.StartStreaming(); // 在这里　开始触发　底层采集的
    
    
    // setup the preview window geometry in order to use it to zoom the image
    // 设置预览窗口几何图形以便使用它缩放图像
    if (mWin != 0) {
        ALOGD("CameraHardware::setPreviewWindow - Negotiating preview format");
        NegotiatePreviewFormat(mWin);
    }

    ALOGD("CameraHardware::startPreviewLocked: starting PreviewThread");
    mPreviewThread = new PreviewThread(this); // 这里new 了一个线程，但并没有启动，那是在那启动的呢： 自己会run

    ALOGD("CameraHardware::startPreviewLocked: O - this:0x%p",this);
    return NO_ERROR;
}
```

### 1. 啥时候run PreviewThread

这里的答案是，在 他new 的时候，就会自动run。 这是怎么做到的呢：

```cpp
void CameraHardware::PreviewThread::onFirstRef()        
{
    run("CameraPreviewThread", PRIORITY_URGENT_DISPLAY); // 优先于 紧急窗口显示
}
// 在第一次给 mPreviewThread = 赋值的时候，根据强引用，第一次会调用这里，此时被run了起来
```

### 2. VIDIOC_S_FMT

```shell
V4L2Camera::Init
	-> ioctl(fd, VIDIOC_S_FMT, &videoIn->format)
```



```cpp
// V4L2Camera.cpp
int V4L2Camera::Init(int width, int height, int fps)
{
    ALOGD("V4L2Camera::Init");

    /* Initialize the capture to the specified width and height */
    // 将捕获初始化为指定的宽度和高度
    static const struct {
        int fmt;            /* PixelFormat */
        int bpp;            /* 每个像素所占几个字节 */
        int isplanar;       /*是否时平面格式 */
        int allowscrop;     /*如果我们支持使用这种像素格式裁剪 */
    } pixFmtsOrder[] = {
        {V4L2_PIX_FMT_YUYV,     2,0,1},
        {V4L2_PIX_FMT_YVYU,     2,0,1},
        {V4L2_PIX_FMT_UYVY,     2,0,1},
        {V4L2_PIX_FMT_YYUV,     2,0,1},
        {V4L2_PIX_FMT_SPCA501,  2,0,0},
        {V4L2_PIX_FMT_SPCA505,  2,0,0},
        {V4L2_PIX_FMT_SPCA508,  2,0,0},
        {V4L2_PIX_FMT_YUV420,   0,1,0},
        {V4L2_PIX_FMT_YVU420,   0,1,0},
        {V4L2_PIX_FMT_NV12,     0,1,0},
        {V4L2_PIX_FMT_NV21,     0,1,0},
        {V4L2_PIX_FMT_NV16,     0,1,0},
        {V4L2_PIX_FMT_NV61,     0,1,0},
        {V4L2_PIX_FMT_Y41P,     0,0,0},
        {V4L2_PIX_FMT_SGBRG8,   0,0,0},
        {V4L2_PIX_FMT_SGRBG8,   0,0,0},
        {V4L2_PIX_FMT_SBGGR8,   0,0,0},
        {V4L2_PIX_FMT_SRGGB8,   0,0,0},
        {V4L2_PIX_FMT_BGR24,    3,0,1},
        {V4L2_PIX_FMT_RGB24,    3,0,1},
        {V4L2_PIX_FMT_MJPEG,    0,1,0},
        {V4L2_PIX_FMT_JPEG,     0,1,0},
        {V4L2_PIX_FMT_GREY,     1,0,1},
        {V4L2_PIX_FMT_Y16,      2,0,1},
    };

    int ret;


    // Try to get the closest match ...
    // 试着找到最接近的匹配
    SurfaceDesc closest;
    int closestDArea = -1;
    int closestDFps = -1;
    unsigned int i;
    int area = width * height;
    for (i = 0; i < m_AllFmts.size(); i++) {
        SurfaceDesc sd = m_AllFmts[i];

        // Always choose a bigger or equal surface 总是选择一个 >= 的 surface
        if (sd.getWidth() >= width &&
            sd.getHeight() >= height) {

            int difArea = sd.getArea() - area;
            int difFps = my_abs(sd.getFps() - fps);
			
            // Trying format: (640 x 480), Fps: 30 [difArea:0, difFps:0, cDifArea:-1, cDifFps:-1]
            ALOGD("Trying format: (%d x %d), Fps: %d [difArea:%d, difFps:%d, cDifArea:%d, cDifFps:%d]",sd.getWidth(),sd.getHeight(),sd.getFps(), difArea, difFps, closestDArea, closestDFps);
            if (closestDArea < 0 ||
                difArea < closestDArea ||
                (difArea == closestDArea && difFps < closestDFps)) {

                // Store approximation
                closestDArea = difArea;
                closestDFps = difFps;

                // And the new surface descriptor
                closest = sd;
            }
        }
    }

	// Selected format: (640 x 480), Fps: 30
    ALOGD("Selected format: (%d x %d), Fps: %d",closest.getWidth(),closest.getHeight(),closest.getFps());

    // Check if we will have to crop the captured image
    bool crop = width != closest.getWidth() || height != closest.getHeight();

    // Iterate through pixel formats from best to worst
    // 从最佳到最差迭代像素格式
    ret = -1;
    for (i=0; i < (sizeof(pixFmtsOrder) / sizeof(pixFmtsOrder[0])); i++) {

        // If we will need to crop, make sure to only select formats we can crop...
        // 如果我们需要裁剪，请确保只选择可以裁剪的格式。。。
        if (!crop || pixFmtsOrder[i].allowscrop) {
            memset(&videoIn->format,0,sizeof(videoIn->format));
            videoIn->format.type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
            videoIn->format.fmt.pix.width = closest.getWidth();
            videoIn->format.fmt.pix.height = closest.getHeight();
            videoIn->format.fmt.pix.pixelformat = pixFmtsOrder[i].fmt;

            ret = ioctl(fd, VIDIOC_TRY_FMT, &videoIn->format); // 尝试是否支持这种格式，
            if (ret >= 0 &&
                videoIn->format.fmt.pix.width ==  (uint)closest.getWidth() &&
                videoIn->format.fmt.pix.height == (uint)closest.getHeight()) {
                break; // 如果尝试的格式和 选择的格式 时一致的，表示 kernel dri 支持这种格式
            } // if
        }// if 结束
    }// for
    
    if (ret < 0) {
        ALOGE("Open: VIDIOC_TRY_FMT Failed: %s", strerror(errno));
        return ret;
    }

    /* Set the format */
    memset(&videoIn->format,0,sizeof(videoIn->format));
    videoIn->format.type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
    videoIn->format.fmt.pix.width = closest.getWidth();
    videoIn->format.fmt.pix.height = closest.getHeight();
    videoIn->format.fmt.pix.pixelformat = pixFmtsOrder[i].fmt;
    ret = ioctl(fd, VIDIOC_S_FMT, &videoIn->format); // 之前是  VIDIOC_TRY_FMT，这里是正式 VIDIOC_S_FMT
    if (ret < 0) {
        ALOGE("Open: VIDIOC_S_FMT Failed: %s", strerror(errno));
        return ret;
    }


    /* Query for the effective video format used */
    // 查询使用的有效视频格式 
    memset(&videoIn->format,0,sizeof(videoIn->format));
    videoIn->format.type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
    ret = ioctl(fd, VIDIOC_G_FMT, &videoIn->format);
    if (ret < 0) {
        ALOGE("Open: VIDIOC_G_FMT Failed: %s", strerror(errno));
        return ret;
    }

    /* Note VIDIOC_S_FMT may change width and height. */
    // 注：VIDIOC_S_FMT可能会改变宽度和高度。

    /* Buggy driver paranoia. */
    unsigned int min = videoIn->format.fmt.pix.width * 2;
    if (videoIn->format.fmt.pix.bytesperline < min)
        videoIn->format.fmt.pix.bytesperline = min;
    min = videoIn->format.fmt.pix.bytesperline * videoIn->format.fmt.pix.height;
    if (videoIn->format.fmt.pix.sizeimage < min)
        videoIn->format.fmt.pix.sizeimage = min;

    /* Store the pixel formats we will use */
    // 存储我们将使用的像素格式
    videoIn->outWidth           = width;
    videoIn->outHeight          = height;
    
    // Calculate the expected output framesize in YUYV 
    // 计算YUYV中的预期输出帧大小
    videoIn->outFrameSize       = width * height << 1;
    videoIn->capBytesPerPixel   = pixFmtsOrder[i].bpp;

    /* Now calculate cropping margins, if needed, rounding to even 
    	现在，如果需要，计算裁剪边距，四舍五入为偶数
    */
    int startX = ((closest.getWidth() - width) >> 1) & (-2);
    int startY = ((closest.getHeight() - height) >> 1) & (-2);

    /* Avoid crashing if the mode found is smaller than the requested
    	如果找到的模式小于请求的模式，请避免崩溃
    */
    if (startX < 0) {
        videoIn->outWidth += startX;
        startX = 0;
    }
    if (startY < 0) {
        videoIn->outHeight += startY;
        startY = 0;
    }

    /* Calculate the starting offset into each captured frame 
    	计算每个捕获帧的起始偏移量
    	*/
    videoIn->capCropOffset = (startX * videoIn->capBytesPerPixel) +
            (videoIn->format.fmt.pix.bytesperline * startY);

    // Cropping from origin: 0x0 - size: 640x480  (offset:0)
    ALOGI("Cropping from origin: %dx%d - size: %dx%d  (offset:%d)",
        startX,startY,
        videoIn->outWidth,videoIn->outHeight,
        videoIn->capCropOffset);

    /* sets video device frame rate 
    	设置视频设备帧速率
    */
    memset(&videoIn->params,0,sizeof(videoIn->params));
    videoIn->params.type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
    videoIn->params.parm.capture.timeperframe.numerator = 1;
    videoIn->params.parm.capture.timeperframe.denominator = closest.getFps();

    /* Set the framerate. If it fails, it wont be fatal 
    	设置帧速率。如果失败了，他不是致命的
    */
    if (ioctl(fd,VIDIOC_S_PARM,&videoIn->params) < 0) {
        ALOGE("VIDIOC_S_PARM error: Unable to set %d fps", closest.getFps());
    }

    /* Gets video device defined frame rate (not real - consider it a maximum value) 
    	获取视频设备定义的帧速率（不真实-认为它是最大值）
    */
    if (ioctl(fd,VIDIOC_G_PARM,&videoIn->params) < 0) {
        ALOGE("VIDIOC_G_PARM - Unable to get timeperframe");
    }

    // Actual format: (640 x 480), Fps: 30, pixfmt: 'YUYV', bytesperline: 1280
    ALOGI("Actual format: (%d x %d), Fps: %d, pixfmt: '%c%c%c%c', bytesperline: %d",
        videoIn->format.fmt.pix.width,
        videoIn->format.fmt.pix.height,
        videoIn->params.parm.capture.timeperframe.denominator,
        videoIn->format.fmt.pix.pixelformat & 0xFF, (videoIn->format.fmt.pix.pixelformat >> 8) & 0xFF,
        (videoIn->format.fmt.pix.pixelformat >> 16) & 0xFF, (videoIn->format.fmt.pix.pixelformat >> 24) & 0xFF,
        videoIn->format.fmt.pix.bytesperline);

    /* Configure JPEG quality, if dealing with those formats 
    	配置JPEG质量，如果处理这些格式
    */
    if (videoIn->format.fmt.pix.pixelformat == V4L2_PIX_FMT_JPEG ||
        videoIn->format.fmt.pix.pixelformat == V4L2_PIX_FMT_MJPEG) {

        /* Get the compression format 
        	获取压缩格式
        */
        ioctl(fd,VIDIOC_G_JPEGCOMP, &videoIn->jpegcomp);

        /* Set to maximum 
        */
        videoIn->jpegcomp.quality = 100;

        /* Try to set it */
        if(ioctl(fd,VIDIOC_S_JPEGCOMP, &videoIn->jpegcomp) >= 0)
        {
            ALOGE("VIDIOC_S_COMP:");
            if(errno == EINVAL)
            {
                videoIn->jpegcomp.quality = -1; //not supported
                ALOGE("   compression control not supported\n");
            }
        }

        /* gets video stream jpeg compression parameters 
        	获取视频流jpeg压缩参数
        */
        if(ioctl(fd,VIDIOC_G_JPEGCOMP, &videoIn->jpegcomp) >= 0) {
            ALOGD("VIDIOC_G_COMP:\n");
            ALOGD("    quality:      %i\n", videoIn->jpegcomp.quality);
            ALOGD("    APPn:         %i\n", videoIn->jpegcomp.APPn);
            ALOGD("    APP_len:      %i\n", videoIn->jpegcomp.APP_len);
            ALOGD("    APP_data:     %s\n", videoIn->jpegcomp.APP_data);
            ALOGD("    COM_len:      %i\n", videoIn->jpegcomp.COM_len);
            ALOGD("    COM_data:     %s\n", videoIn->jpegcomp.COM_data);
            ALOGD("    jpeg_markers: 0x%x\n", videoIn->jpegcomp.jpeg_markers);
        } else {
            ALOGE("VIDIOC_G_COMP:");
            if(errno == EINVAL) {
                videoIn->jpegcomp.quality = -1; //not supported
                ALOGE("   compression control not supported\n");
            }
        }
    }

    /* Check if camera can handle NB_BUFFER buffers 
    	检查摄像机是否能处理NB_缓冲区
    */
    memset(&videoIn->rb,0,sizeof(videoIn->rb));
    videoIn->rb.type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
    videoIn->rb.memory = V4L2_MEMORY_MMAP;
    videoIn->rb.count = NB_BUFFER;

    ret = ioctl(fd, VIDIOC_REQBUFS, &videoIn->rb);
    if (ret < 0) {
        ALOGE("Init: VIDIOC_REQBUFS failed: %s", strerror(errno));
        return ret;
    }

    for (int i = 0; i < NB_BUFFER; i++) {

        memset (&videoIn->buf, 0, sizeof (struct v4l2_buffer));
        videoIn->buf.index = i;
        videoIn->buf.type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
        videoIn->buf.memory = V4L2_MEMORY_MMAP;

        
        
        
        // 申请缓存
        ret = ioctl (fd, VIDIOC_QUERYBUF, &videoIn->buf);
        if (ret < 0) {
            ALOGE("Init: Unable to query buffer (%s)", strerror(errno));
            return ret;
        }

        
        // mmap  将申请的内存，映射到用户空间
        // 将映射后的虚拟地址保存在 videoIn->mem[i] 中
        videoIn->mem[i] = mmap (0,
                                videoIn->buf.length,
                                PROT_READ | PROT_WRITE,
                                MAP_SHARED,
                                fd,
                                videoIn->buf.m.offset);

        if (videoIn->mem[i] == MAP_FAILED) {
            ALOGE("Init: Unable to map buffer (%s)", strerror(errno));
            return -1;
        }

        
        // 将申请问 并mmap 后的缓存，放入　，内核中的　内核中的　缓存队列中，准备采集
        ret = ioctl(fd, VIDIOC_QBUF, &videoIn->buf);
        if (ret < 0) {
            ALOGE("Init: VIDIOC_QBUF Failed");
            return -1;
        }

        nQueued++;
    }

    // Reserve temporary buffers, if they will be needed
    // 如果需要，请保留临时缓冲区
    size_t tmpbuf_size=0;
    switch (videoIn->format.fmt.pix.pixelformat)
    {
        case V4L2_PIX_FMT_JPEG:
        case V4L2_PIX_FMT_MJPEG:
        case V4L2_PIX_FMT_UYVY:
        case V4L2_PIX_FMT_YVYU:
        case V4L2_PIX_FMT_YYUV:
        case V4L2_PIX_FMT_YUV420: // only needs 3/2 bytes per pixel but we alloc 2 bytes per pixel
        case V4L2_PIX_FMT_YVU420: // only needs 3/2 bytes per pixel but we alloc 2 bytes per pixel
        case V4L2_PIX_FMT_Y41P:   // only needs 3/2 bytes per pixel but we alloc 2 bytes per pixel
        case V4L2_PIX_FMT_NV12:
        case V4L2_PIX_FMT_NV21:
        case V4L2_PIX_FMT_NV16:
        case V4L2_PIX_FMT_NV61:
        case V4L2_PIX_FMT_SPCA501:
        case V4L2_PIX_FMT_SPCA505:
        case V4L2_PIX_FMT_SPCA508:
        case V4L2_PIX_FMT_GREY:
        case V4L2_PIX_FMT_Y16:

        case V4L2_PIX_FMT_YUYV:
            //  YUYV doesn't need a temp buffer but we will set it if/when
            //  video processing disable control is checked (bayer processing).
            //            (logitech cameras only)
            // YUYV不需要临时缓冲区，但如果/当选中视频处理禁用控制（拜耳处理）时，我们将设置它。（仅限罗技摄像机）
            break;

        case V4L2_PIX_FMT_SGBRG8: //0
        case V4L2_PIX_FMT_SGRBG8: //1
        case V4L2_PIX_FMT_SBGGR8: //2
        case V4L2_PIX_FMT_SRGGB8: //3
            // Raw 8 bit bayer
            // when grabbing use:
            //    bayer_to_rgb24(bayer_data, RGB24_data, width, height, 0..3)
            //    rgb2yuyv(RGB24_data, pFrameBuffer, width, height)

            // alloc a temp buffer for converting to YUYV
            // 分配转换为YUYV的临时缓冲区
            
            // rgb buffer for decoding bayer data 
            // 解码bayer数据的rgb缓冲区
            tmpbuf_size = videoIn->format.fmt.pix.width * videoIn->format.fmt.pix.height * 3;
            if (videoIn->tmpBuffer)
                free(videoIn->tmpBuffer);
            videoIn->tmpBuffer = (uint8_t*)calloc(1, tmpbuf_size);
            if (!videoIn->tmpBuffer) {
                ALOGE("couldn't calloc %lu bytes of memory for frame buffer\n",
                    (unsigned long) tmpbuf_size);
                return -ENOMEM;
            }


            break;

        case V4L2_PIX_FMT_RGB24: //rgb or bgr (8-8-8)
        case V4L2_PIX_FMT_BGR24:
            break;

        default:
            ALOGE("Should never arrive (1)- exit fatal !!\n");
            return -1;
    }

    return 0;
}
```

### 3. VIDIOC_QUERYBUF

在 2.3.2 中，在 分析 Init()中可以看到 调用

```shell
V4L2Camera::Init
	-> ioctl(fd, VIDIOC_S_FMT, &videoIn->format)
	-> ioctl (fd, VIDIOC_QUERYBUF, &videoIn->buf);
	
```

```cpp
V4L2Camera::Init(){
    ...
     // 申请缓存
        ret = ioctl (fd, VIDIOC_QUERYBUF, &videoIn->buf);
        if (ret < 0) {
            ALOGE("Init: Unable to query buffer (%s)", strerror(errno));
            return ret;
        }

    ...
}
```



### 4. mmap

将申请的内存，映射到用户空间

在 2.3.2 中，在 分析 Init()中可以看到 调用

```shell
V4L2Camera::Init
	-> ioctl(fd, VIDIOC_S_FMT, &videoIn->format)
	-> ioctl (fd, VIDIOC_QUERYBUF, &videoIn->buf);
	-> mmap ()
```

```cpp
V4L2Camera::Init(){
    ...
     
        // mmap  将申请的内存，映射到用户空间
        // 将映射后的虚拟地址保存在 videoIn->mem[i] 中
        videoIn->mem[i] = mmap (0,
                                videoIn->buf.length,
                                PROT_READ | PROT_WRITE,
                                MAP_SHARED,
                                fd,
                                videoIn->buf.m.offset);

        if (videoIn->mem[i] == MAP_FAILED) {
            ALOGE("Init: Unable to map buffer (%s)", strerror(errno));
            return -1;
        }
    ...
}
```

videoIn 是  

```cpp
class V4L2Camera { 
    private:
    	struct vdIn *videoIn;
}
```

### 5. VIDIOC_QBUF

```shell
V4L2Camera::Init
	-> ioctl(fd, VIDIOC_S_FMT, &videoIn->format)
	-> ioctl (fd, VIDIOC_QUERYBUF, &videoIn->buf)
	-> mmap ()
	->  ioctl(fd, VIDIOC_QBUF, &videoIn->buf)
```



```cpp
V4L2Camera::Init(){
    ...
     
          // 将申请问 并mmap 后的缓存，放入　，内核中的　内核中的　缓存队列中，准备采集
        ret = ioctl(fd, VIDIOC_QBUF, &videoIn->buf);
        if (ret < 0) {
            ALOGE("Init: VIDIOC_QBUF Failed");
            return -1;
        }

        nQueued++;
    ...
}
```

## ４．采集

### 1.  VIDIOC_STREAMON

```cpp
 CameraHardware::setPreviewWindow
 CameraHardware::startPreview
  CameraHardware::startPreviewLocked
  CameraHardware::startPreviewLocked: Open, 640x480
		-> V4L2Camera::EnumFrameFormats  
      
 CameraHardware::startPreviewLocked: Init
		-> V4L2Camera::Init()
    			-> ioctl(fd, VIDIOC_S_FMT, &videoIn->format)
     			-> ioctl (fd, VIDIOC_QUERYBUF, &videoIn->buf); // 申请缓存
				-> mmap() // 映射到用户空间
                ->  ioctl(fd, VIDIOC_QBUF, &videoIn->buf) // 加入kenrl 中的　缓存队列中，准备采集
     	-> initHeapLocked
     	-> camera.StartStreaming(); // 开始采集
```

```cpp
int V4L2Camera::StartStreaming ()
{
    enum v4l2_buf_type type;                                                                                                                                                                                       
    int ret;

    if (!videoIn->isStreaming) {
        type = V4L2_BUF_TYPE_VIDEO_CAPTURE;

        ret = ioctl (fd, VIDIOC_STREAMON, &type); // 开始采集数据
        if (ret < 0) {
            ALOGE("StartStreaming: Unable to start capture: %s", strerror(errno));
            return ret;
        }

        videoIn->isStreaming = true;
    }   

    return 0;
}
```

### 2. VIDIOC_DQBUF



GrabRawFrame　函数会在　两个线程中调用：

1. previewThread 是在　startPreviewLocked() 中启动的
2. pictureThread　是在　takePicture()中启动的

```shell
# 第二种方式的调用
CameraHardware::takePicture
	->  createThread(beginPictureThread, this) # 此时启动了一个线程
	
CameraHardware::beginPictureThread
	->  pictureThread()
			-> camera.GrabRawFrame()
```





从底层缓存队列中拿出获得的图像(帧)

```shell
V4L2Camera::GrabRawFrame
	-> ioctl(fd, VIDIOC_DQBUF, &videoIn->buf) # 取出buf
	-> jpeg_decode() # 解码
	-> ioctl(fd, VIDIOC_QBUF, &videoIn->buf) # 将　buf　放入　kernel中的队列中 
```

```cpp
/* Grab frame in YUYV mode */
void V4L2Camera::GrabRawFrame (void *frameBuffer, int maxSize)
{
    // V4L2Camera::GrabRawFrame: frameBuffer:0xf326a000, len:614400
    LOG_FRAME("V4L2Camera::GrabRawFrame: frameBuffer:%p, len:%d",frameBuffer,maxSize);
    int ret;

    /* DQ */
    memset(&videoIn->buf,0,sizeof(videoIn->buf));
    videoIn->buf.type = V4L2_BUF_TYPE_VIDEO_CAPTURE;
    videoIn->buf.memory = V4L2_MEMORY_MMAP;
    // 从kernel 中的缓存队列中拿出一个　帧
    ret = ioctl(fd, VIDIOC_DQBUF, &videoIn->buf);
    if (ret < 0) {
        ALOGE("GrabPreviewFrame: VIDIOC_DQBUF Failed");
        return;
    }

    nDequeued++;


    // 拿到图片的　虚拟地址
    uint8_t* src = (uint8_t*)videoIn->mem[videoIn->buf.index] + videoIn->capCropOffset;

    //  V4L2Camera::GrabRawFrame - Got Raw frame (640x480) (buf:0@0x0xf2d24000, len:614400)
    LOG_FRAME("V4L2Camera::GrabRawFrame - Got Raw frame (%dx%d) (buf:%d@0x%p, len:%d)",videoIn->format.fmt.pix.width,videoIn->format.fmt.pix.height,videoIn->buf.index,src,videoIn->buf.bytesused);

    /* Avoid crashing! - Make sure there is enough room in the output buffer! */
    if (maxSize < videoIn->outFrameSize) {

        ALOGE("V4L2Camera::GrabRawFrame: Insufficient space in output buffer: Required: %d, Got %d - DROPPING FRAME",videoIn->outFrameSize,maxSize);

    } else {

        switch (videoIn->format.fmt.pix.pixelformat)
        {
            case V4L2_PIX_FMT_JPEG:
            case V4L2_PIX_FMT_MJPEG:
                if(videoIn->buf.bytesused <= HEADERFRAME1) {
                    // Prevent crash on empty image
                    ALOGE("Ignoring empty buffer ...\n");
                    break;
                }
				// jpeg格式的解码
                if (jpeg_decode((uint8_t*)frameBuffer, strideOut, src, videoIn->outWidth, videoIn->outHeight) < 0) {
                    ALOGE("jpeg decode errors\n");
                    break;
                }
                break;

            case V4L2_PIX_FMT_UYVY:
                uyvy_to_yuyv((uint8_t*)frameBuffer, strideOut,
                             src, videoIn->format.fmt.pix.bytesperline, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_YVYU:
                yvyu_to_yuyv((uint8_t*)frameBuffer, strideOut,
                             src, videoIn->format.fmt.pix.bytesperline, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_YYUV:
                yyuv_to_yuyv((uint8_t*)frameBuffer, strideOut,
                             src, videoIn->format.fmt.pix.bytesperline, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_YUV420:
                yuv420_to_yuyv((uint8_t*)frameBuffer, strideOut, src, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_YVU420:
                yvu420_to_yuyv((uint8_t*)frameBuffer, strideOut, src, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_NV12:
                nv12_to_yuyv((uint8_t*)frameBuffer, strideOut, src, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_NV21:
                nv21_to_yuyv((uint8_t*)frameBuffer, strideOut, src, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_NV16:
                nv16_to_yuyv((uint8_t*)frameBuffer, strideOut, src, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_NV61:
                nv61_to_yuyv((uint8_t*)frameBuffer, strideOut, src, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_Y41P:
                y41p_to_yuyv((uint8_t*)frameBuffer, strideOut, src, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_GREY:
                grey_to_yuyv((uint8_t*)frameBuffer, strideOut,
                            src, videoIn->format.fmt.pix.bytesperline, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_Y16:
                y16_to_yuyv((uint8_t*)frameBuffer, strideOut,
                            src, videoIn->format.fmt.pix.bytesperline, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_SPCA501:
                s501_to_yuyv((uint8_t*)frameBuffer, strideOut, src, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_SPCA505:
                s505_to_yuyv((uint8_t*)frameBuffer, strideOut, src, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_SPCA508:
                s508_to_yuyv((uint8_t*)frameBuffer, strideOut, src, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_YUYV:
                {
                    int h;
                    uint8_t* pdst = (uint8_t*)frameBuffer;
                    uint8_t* psrc = src;
                    int ss = videoIn->outWidth << 1;
                    for (h = 0; h < videoIn->outHeight; h++) {
                        memcpy(pdst,psrc,ss);
                        pdst += strideOut;
                        psrc += videoIn->format.fmt.pix.bytesperline;
                    }
                }
                break;

            case V4L2_PIX_FMT_SGBRG8: //0
                bayer_to_rgb24 (src,(uint8_t*) videoIn->tmpBuffer, videoIn->outWidth, videoIn->outHeight, 0);
                rgb_to_yuyv ((uint8_t*) frameBuffer, strideOut,
                            (uint8_t*)videoIn->tmpBuffer, videoIn->outWidth*3, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_SGRBG8: //1
                bayer_to_rgb24 (src,(uint8_t*) videoIn->tmpBuffer, videoIn->outWidth, videoIn->outHeight, 1);
                rgb_to_yuyv ((uint8_t*) frameBuffer, strideOut,
                            (uint8_t*)videoIn->tmpBuffer, videoIn->outWidth*3, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_SBGGR8: //2
                bayer_to_rgb24 (src,(uint8_t*) videoIn->tmpBuffer, videoIn->outWidth, videoIn->outHeight, 2);
                rgb_to_yuyv ((uint8_t*) frameBuffer, strideOut,
                            (uint8_t*)videoIn->tmpBuffer, videoIn->outWidth*3, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_SRGGB8: //3
                bayer_to_rgb24 (src,(uint8_t*) videoIn->tmpBuffer, videoIn->outWidth, videoIn->outHeight, 3);
                rgb_to_yuyv ((uint8_t*) frameBuffer, strideOut,
                            (uint8_t*)videoIn->tmpBuffer, videoIn->outWidth*3, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_RGB24:
                rgb_to_yuyv((uint8_t*) frameBuffer, strideOut,
                            src, videoIn->format.fmt.pix.bytesperline, videoIn->outWidth, videoIn->outHeight);
                break;

            case V4L2_PIX_FMT_BGR24:
                bgr_to_yuyv((uint8_t*) frameBuffer, strideOut,
                            src, videoIn->format.fmt.pix.bytesperline, videoIn->outWidth, videoIn->outHeight);
                break;

            default:
                ALOGE("error grabbing: unknown format: %i\n", videoIn->format.fmt.pix.pixelformat);
                break;
        }

        LOG_FRAME("V4L2Camera::GrabRawFrame - Copied frame to destination 0x%p",frameBuffer);
    }

    /* And Queue the buffer again */
    // 将空的　buf 加入队列
    ret = ioctl(fd, VIDIOC_QBUF, &videoIn->buf);
    if (ret < 0) {
        ALOGE("GrabPreviewFrame: VIDIOC_QBUF Failed");
        return;
    }

    nQueued++;

    LOG_FRAME("V4L2Camera::GrabRawFrame - Queued buffer");

}
```

## 5.  previewThread

在前面的　2.3.1 小结中，已经说明白了什么时候启动　previewThread

在这里分析　previewThread　都做了那些事

```shell

```

源代码：

```cpp
bool CameraHardware::PreviewThread::threadLoop()
{
    mHardware->previewThread();
    // loop until we need to quit
    return true;                                                                                                                                                                                                   
}
```

从　threadLoop　我们可以知道，这个循环会调用　mHardware->previewThread();

```cpp
int CameraHardware::previewThread()
{
    // CameraHardware::previewThread: this=0xf37cf000
    ALOGV("CameraHardware::previewThread: this=%p",this);

    int previewFrameRate = mParameters.getPreviewFrameRate();

    // Calculate how long to wait between frames.
    // 计算帧之间的等待时间。
    int delay = (int)(1000000 / previewFrameRate);

    // Buffers to send messages
    int recBufferIdx = 0;
    int previewBufferIdx = 0;

    bool record = false;
    bool preview = false;

    // Get the current timestamp
    nsecs_t timestamp = systemTime(SYSTEM_TIME_MONOTONIC);

    // We must avoid a race condition here when destroying the thread...
    //  So, if we fail to lock the mutex, just retry a bit later, but
    //  let the android thread end if requested!
    // 我们必须避免在这里破坏线程的种族状况。。。所以，如果锁定互斥锁失败，请稍后重试，但如果请求，让android线程结束！
    if (mLock.tryLock() == NO_ERROR) {

        // If no raw preview buffer, we can't do anything...
        // 如果没有原始预览缓冲区，我们将无法执行任何操作。。。
        if (mRawPreviewBuffer == 0) {
            ALOGE("No Raw preview buffer!");
            mLock.unlock();
            return NO_ERROR;
        }

        // Get the preview buffer for the current frame
        // This is always valid, even if the client died -- the memory
        // is still mapped in our process.
        
     //  获取当前帧的预览缓冲区
	//这总是有效的，即使客户端死了——内存
	//仍在我们的流程中。
        uint8_t *frame = (uint8_t *)mPreviewBuffer[mCurrentPreviewFrame];

        // If no preview buffer, we cant do anything...
        if (frame == 0) {
            ALOGE("No preview buffer!");
            mLock.unlock();
            return NO_ERROR;
        }


        //  Get a pointer to the memory area to use... In case of previewing in YUV422I, we
        // can save a buffer copy by directly using the output buffer. But ONLY if NOT recording
        // or, in case of recording, when size matches
        /*
        	获取指向要使用的内存区域的指针。。。
        	在YUV422I中预览时，可以直接使用输出缓冲区来保存缓冲区副本但只有在不录音的情况下，
        	或者在录音的情况下，当尺寸匹配时
        */
        uint8_t* rawBase = (mPreviewFmt == PIXEL_FORMAT_YCrCb_422_I &&
                            (!mRecordingEnabled || mRawPreviewFrameSize == mPreviewFrameSize))
                            ? frame : (uint8_t*)mRawPreviewBuffer;

        // Grab a frame in the raw format YUYV
        // 获取原始格式的帧YUYV
        camera.GrabRawFrame(rawBase, mRawPreviewFrameSize);

        // If the recording is enabled...
        // 如果录音已启用。。。
        if (mRecordingEnabled && mMsgEnabled & CAMERA_MSG_VIDEO_FRAME) {
            //ALOGD("CameraHardware::previewThread: posting video frame...");

            // Get the video size. We are warrantied here that the current capture
            // size IS exacty equal to the video size, as this condition is enforced
            // by this driver, that priorizes recording size over preview size requirements
            /*
            	获取视频大小。我们在此保证当前捕获大小与视频大小完全相等，
            	因为此条件由该驱动程序强制执行，它优先考虑录制大小而不是预览大小要求
            */

            uint8_t *recFrame = (uint8_t *) mRecBuffers[mCurrentRecordingFrame];
            if (recFrame != 0) {

                // Convert from our raw frame to the one the Record requires
                // 从原始帧转换为记录所需的帧
                switch (mRecFmt) {

                // Note: Apparently, Android's "YCbCr_422_SP" is merely an arbitrary label
                // The preview data comes in a YUV 4:2:0 format, with Y plane, then VU plane
                case PIXEL_FORMAT_YCbCr_422_SP:
                    yuyv_to_yvu420sp(recFrame, mRawPreviewWidth, mRawPreviewHeight, rawBase, (mRawPreviewWidth<<1), mRawPreviewWidth, mRawPreviewHeight);
                    break;

                case PIXEL_FORMAT_YCbCr_420_SP:
                    yuyv_to_yvu420sp(recFrame, mRawPreviewWidth, mRawPreviewHeight, rawBase, (mRawPreviewWidth<<1), mRawPreviewWidth, mRawPreviewHeight);
                    break;

                case PIXEL_FORMAT_YV12:
                    /* OMX recorder needs YUV */
                    yuyv_to_yuv420p(recFrame, mRawPreviewWidth, mRawPreviewHeight, rawBase, (mRawPreviewWidth<<1), mRawPreviewWidth, mRawPreviewHeight);
                    break;

                case PIXEL_FORMAT_YCrCb_422_I:
                    memcpy(recFrame, rawBase, mRecordingFrameSize);
                    break;
                }

                // Remember we must schedule the callback
                record = true;

                // Advance the buffer pointer.
                recBufferIdx = mCurrentRecordingFrame;
                mCurrentRecordingFrame = (mCurrentRecordingFrame + 1) % kBufferCount;
            }
        }

        if (mMsgEnabled & CAMERA_MSG_PREVIEW_FRAME) {
            //ALOGD("CameraHardware::previewThread: posting preview frame...");

            // Here we could eventually have a problem: If we are recording, the recording size
            //  takes precedence over the preview size. So, the rawBase buffer could be of a
            //  different size than the preview buffer. Handle this situation by centering/cropping
            //  if needed.

            // Get the preview size
            int width = 0, height = 0;
            mParameters.getPreviewSize(&width,&height);

            // Assume we will be able to copy at least those pixels
            int cwidth = width;
            int cheight = height;

            // If we are trying to display a preview larger than the effective capture, truncate to it
            if (cwidth > mRawPreviewWidth)
                cwidth = mRawPreviewWidth;
            if (cheight > mRawPreviewHeight)
                cheight = mRawPreviewHeight;

            // Convert from our raw frame to the one the Preview requires
            switch (mPreviewFmt) {

                // Note: Apparently, Android's "YCbCr_422_SP" is merely an arbitrary label
                // The preview data comes in a YUV 4:2:0 format, with Y plane, then VU plane
            case PIXEL_FORMAT_YCbCr_422_SP: // This is misused by android...
                yuyv_to_yvu420sp(frame, width, height, rawBase, (mRawPreviewWidth<<1), cwidth, cheight);
                break;

            case PIXEL_FORMAT_YCbCr_420_SP:
                yuyv_to_yvu420sp(frame, width, height, rawBase, (mRawPreviewWidth<<1), cwidth, cheight);
                break;

            case PIXEL_FORMAT_YV12:
                yuyv_to_yvu420p(frame, width, height, rawBase, (mRawPreviewWidth<<1), cwidth, cheight);
                break;

            case PIXEL_FORMAT_YCrCb_422_I:
                // Nothing to do here. Is is handled as a special case without buffer copies...
                //  but ONLY in special cases... Otherwise, handle the copy!
                if (mRecordingEnabled && mRawPreviewFrameSize != mPreviewFrameSize) {
                    // We need to copy ... do it
                    uint8_t* dst = frame;
                    uint8_t* src = rawBase;
                    int h;
                    for (h = 0; h < cheight; h++) {
                        memcpy(dst,src,cwidth<<1);
                        dst += width << 1;
                        src += mRawPreviewWidth<<1;
                    }
                }
                break;

            default:
                ALOGE("Unhandled pixel format");

            }

            // Remember we must schedule the callback
            preview = true;

            // Advance the buffer pointer.
            previewBufferIdx = mCurrentPreviewFrame;
            mCurrentPreviewFrame = (mCurrentPreviewFrame + 1) % kBufferCount;
        }

        // Display the preview image
        // 显示预览图像
        fillPreviewWindow(rawBase, mRawPreviewWidth, mRawPreviewHeight);

        // Release the lock
        mLock.unlock();

    } else {

        // Delay a little ... and reattempt the lock on the next iteration
        delay >>= 7;
    }

    // We must schedule the callbacks Outside the lock, or the caller
    //  could call us and cause a deadlock!
    if (preview) {
        mDataCb(CAMERA_MSG_PREVIEW_FRAME, mPreviewHeap, previewBufferIdx, NULL, mCallbackCookie);
    }

    if (record) {
        // Record callback uses a timestamped frame
        mDataCbTimestamp(timestamp, CAMERA_MSG_VIDEO_FRAME, mRecordingHeap, recBufferIdx, mCallbackCookie);
    }

    ALOGV("previewThread OK");

    // Wait for it...
    usleep(delay);

    return NO_ERROR;
}
```

## 6. VIDIOC_STREAMOFF

```shell
# 退出预览时会　停止
CameraHardware::stopPreviewLocked
	-> camera.StopStreaming();
	
# 拍完照片后会停止
CameraHardware::pictureThread
	-> camera.StopStreaming();
```

```cpp
int V4L2Camera::StopStreaming ()
{
    enum v4l2_buf_type type;
    int ret;

    if (videoIn->isStreaming) {
        type = V4L2_BUF_TYPE_VIDEO_CAPTURE;

        ret = ioctl (fd, VIDIOC_STREAMOFF, &type);
        if (ret < 0) {                                                                                                                                                                                             
            ALOGE("StopStreaming: Unable to stop capture: %s", strerror(errno));
            return ret;
        }

        videoIn->isStreaming = false;
    }   

    return 0;
}

```

